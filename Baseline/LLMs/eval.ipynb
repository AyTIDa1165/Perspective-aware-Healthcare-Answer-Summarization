{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from bert_score import score as bert_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics:\n",
    "    def __init__(self, predictions, references):\n",
    "        self.predictions = predictions\n",
    "        self.references = references\n",
    "\n",
    "    def compute_rouge_score(self):\n",
    "        rouge = Rouge()\n",
    "        rouge_l_f1, rouge_l_recall, rouge_l_precision = [], [], []\n",
    "        rouge_1_f1, rouge_1_recall, rouge_1_precision = [], [], []\n",
    "        rouge_2_f1, rouge_2_recall, rouge_2_precision = [], [], []\n",
    "        \n",
    "        for prediction, reference in zip(self.predictions, self.references):\n",
    "            scores = rouge.get_scores(prediction, reference)[0]\n",
    "            \n",
    "            rouge_l_f1.append(scores[\"rouge-l\"][\"f\"])\n",
    "            rouge_l_recall.append(scores[\"rouge-l\"][\"r\"])\n",
    "            rouge_l_precision.append(scores[\"rouge-l\"][\"p\"])\n",
    "            \n",
    "            rouge_1_f1.append(scores[\"rouge-1\"][\"f\"])\n",
    "            rouge_1_recall.append(scores[\"rouge-1\"][\"r\"])\n",
    "            rouge_1_precision.append(scores[\"rouge-1\"][\"p\"])\n",
    "            \n",
    "            rouge_2_f1.append(scores[\"rouge-2\"][\"f\"])\n",
    "            rouge_2_recall.append(scores[\"rouge-2\"][\"r\"])\n",
    "            rouge_2_precision.append(scores[\"rouge-2\"][\"p\"])\n",
    "\n",
    "        results = {\n",
    "            \"rouge_l\": {\n",
    "                \"f1\": np.mean(rouge_l_f1) * 100 ,\n",
    "                \"recall\": np.mean(rouge_l_recall) * 100,\n",
    "                \"precision\": np.mean(rouge_l_precision) * 100\n",
    "            },\n",
    "            \"rouge_1\": {\n",
    "                \"f1\": np.mean(rouge_1_f1) * 100,\n",
    "                \"recall\": np.mean(rouge_1_recall) * 100,\n",
    "                \"precision\": np.mean(rouge_1_precision) * 100\n",
    "            },\n",
    "            \"rouge_2\": {\n",
    "                \"f1\": np.mean(rouge_2_f1) * 100,\n",
    "                \"recall\": np.mean(rouge_2_recall) * 100,\n",
    "                \"precision\": np.mean(rouge_2_precision) * 100\n",
    "            }\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def compute_meteor_score(self):\n",
    "        # Using evaluate instead of load_metric\n",
    "        meteor = evaluate.load('meteor')\n",
    "        scores = []\n",
    "        for prediction, reference in zip(self.predictions, self.references):\n",
    "            score = meteor.compute(predictions=[prediction], references=[reference])\n",
    "            scores.append(score[\"meteor\"])\n",
    "\n",
    "        average_meteor_score = np.mean(scores)\n",
    "        return {\"meteor\": average_meteor_score}\n",
    "    \n",
    "    def compute_bleu_scores(self):\n",
    "        bleu_1, bleu_2, bleu_3, bleu_4 = [], [], [], []\n",
    "        smoothie = SmoothingFunction().method4\n",
    "        \n",
    "        for prediction, reference in zip(self.predictions, self.references):\n",
    "            reference = [reference.split()]\n",
    "            prediction = prediction.split()\n",
    "            \n",
    "            bleu_1.append(sentence_bleu(reference, prediction, weights=(1, 0, 0, 0), smoothing_function=smoothie))\n",
    "            bleu_2.append(sentence_bleu(reference, prediction, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie))\n",
    "            bleu_3.append(sentence_bleu(reference, prediction, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie))\n",
    "            bleu_4.append(sentence_bleu(reference, prediction, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie))\n",
    "\n",
    "        results = {\n",
    "            \"bleu_1\": np.mean(bleu_1),\n",
    "            \"bleu_2\": np.mean(bleu_2),\n",
    "            \"bleu_3\": np.mean(bleu_3),\n",
    "            \"bleu_4\": np.mean(bleu_4)\n",
    "        }\n",
    "        return results\n",
    "    \n",
    "    def compute_bertscore(self, lang=\"en\"):\n",
    "        # Compute BERTScore\n",
    "        P, R, F1 = bert_score(self.predictions, self.references, lang=lang)\n",
    "        return {\n",
    "            \"precision\": P.mean().item(),\n",
    "            \"recall\": R.mean().item(),\n",
    "            \"f1\": F1.mean().item()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE scores: {'rouge_l': {'f1': np.float64(28.6694499740343), 'recall': np.float64(41.56357281452255), 'precision': np.float64(25.13207231048138)}, 'rouge_1': {'f1': np.float64(31.574420749000815), 'recall': np.float64(45.5992779821014), 'precision': np.float64(27.66690313739584)}, 'rouge_2': {'f1': np.float64(11.551963701634985), 'recall': np.float64(17.149280106806266), 'precision': np.float64(10.347562875300435)}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR score: {'meteor': np.float64(0.37116963865550623)}\n",
      "BLEU scores: {'bleu_1': np.float64(0.24042566131539708), 'bleu_2': np.float64(0.1424754647714093), 'bleu_3': np.float64(0.09679226641801822), 'bleu_4': np.float64(0.06592722552612827)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load test data and predictions\n",
    "with open('../../Dataset/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "with open('./predictions_gemini.json', 'r') as f:\n",
    "    output_data = json.load(f)\n",
    "\n",
    "# Check for length mismatch\n",
    "if len(test_data) != len(output_data):\n",
    "    print(\"LENGTH ERROR\")\n",
    "    exit(1)\n",
    "\n",
    "predictions, references = [], []\n",
    "for i in range(len(test_data)):\n",
    "    predicted_summaries = output_data[i]['summaries']\n",
    "    reference_summaries = test_data[i]['labelled_summaries']\n",
    "    for perspective in predicted_summaries.keys():\n",
    "        if perspective in reference_summaries:\n",
    "            predictions.append(predicted_summaries[perspective][0])\n",
    "            references.append(reference_summaries[perspective])\n",
    "\n",
    "# Evaluate using the EvaluationMetrics class\n",
    "eval_metrics = EvaluationMetrics(predictions, references)\n",
    "\n",
    "print(\"ROUGE scores:\", eval_metrics.compute_rouge_score())\n",
    "print(\"METEOR score:\", eval_metrics.compute_meteor_score())\n",
    "print(\"BLEU scores:\", eval_metrics.compute_bleu_scores())\n",
    "print(\"BERTScore:\", eval_metrics.compute_bertscore(lang=\"en\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
